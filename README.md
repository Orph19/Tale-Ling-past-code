# Tale Ling
Web app for learning Spanish while reading tailored short stories generated by AI

## Key Features
- Content Discovery & Saving: Users can search for and save books, TV shows, and movies as preferences.
- Personalized Story Generation: The app leverages user preferences, obtained via the Qloo API, to dynamically instruct the LLM to create unique tailored stories.
- Innovative Language Learning: The learning approach focuses on vocabulary, context, and entertainment. Stories are generated in segments, each containing specific Spanish words.
- Interactive Translation: Users can translate any segment of the story to understand the meaning of the words.
- Spaced Repetition System: Users can track words they've encountered and change their status (e.g., from 'Base' to 'Familiar'). 'Familiar' words are repeated in new stories to reinforce learning, while 'Learned' words are no longer shown.
  
## Technologies
- LLM: Story and translation generation is powered by the Gemini 2.5 large language model.
- Backend: The backend is built with Node.js and Express.
- Database & Authentication: Supabase handles the database, user authentication, and row-level security.
- Classification Model: A multi-class classification model for tag classification is hosted on a private Hugging Face Space and accessed via Gradio.
- Frontend: The frontend uses Tailwind CSS, and Markdown is implemented in the story.js file to parse text generated by the Gemini model.

## Local Installation 
1. Clone the repository:
```
git clone https://github.com/Orph19/Tale-Ling.git
```
2. Install dependencies:
```
npm install
```
3. Set up environment variables:
- Create a .env file based on the provided .env.example.
- You will need a Hugging Face token to access the classification model. You can request this token or set up a local server using the Python backend found [here](https://github.com/Orph19/multiClassClassifierModel). The current JavaScript code can be modified to call that local server's endpoint.
- You'll also need to configure Supabase. Create a new project and set up the required tables, or request the project URL and key.
4. Run the application: 
```
npm start
```
## Demo
A demo video of the app can be found [here](https://youtu.be/-3jo9Z0Qhjw).

## Challenges and Future Improvements
- Model Performance: The current Gemini 2.5-flash model struggles with instruction-following and unique story generation. Upgrading to the 2.5-Pro model would improve quality but increase latency.
- Classification Model Accuracy: The multi-class classification model was trained on a small dataset with a ad hoc pipeline. It can be enhanced for better accuracy, which will directly improve story relevance and personalization.
- Data Synchronization: A timing issue currently prevents word groups from being updated correctly if a user moves words while a new story is being generated (e.g., creating duplicate words across different groups). This requires a more robust fetching system or a temporary lock to ensure data consistency.
